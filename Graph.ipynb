{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide the files' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = ''\n",
    "outfile = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the file containing the text\n",
    "\n",
    "with open(infile, 'r') as file:\n",
    "    data = file.read().replace('\\n', ' ')\n",
    "\n",
    "text = re.sub('[^A-Za-z0-9]+', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file containing the characters and put them in a list\n",
    "\n",
    "chars = []\n",
    "\n",
    "with open(\"characters.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        name = line.strip().lower()\n",
    "        chars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokens and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Get a list of stop words\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def wordTokens(text, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenization of a string in word tokens\n",
    "    \n",
    "    :param text: A string\n",
    "    :param stop_words: A list of stop words to not be considered in the final tokens\n",
    "\n",
    "    :return: A list of word tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    wtokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    wtokens = [w for w in wtokens if w not in stop_words]\n",
    "\n",
    "    return wtokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the n-grams\n",
    "\n",
    "tokens = wordTokens(text, stopWords)\n",
    "bigrm = nltk.bigrams(tokens)\n",
    "bigrms = list(bigrm)\n",
    "trigrm = nltk.trigrams(tokens)\n",
    "trigrms = list(trigrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def char_tuple_f(chars_list):\n",
    "    \"\"\"\n",
    "    This function creates a list of tuples from the names of the characters.\n",
    "    \n",
    "    :param chars_list: A list of character names. The names should be in lowercase.\n",
    "    :return: A list of tuples for each name in the initial list\n",
    "    \"\"\"\n",
    "    \n",
    "    char_tuples_list = []\n",
    "    for char in chars:\n",
    "        tup = tuple(char.split(\" \"))\n",
    "        char_tuples_list.append(tup)\n",
    "    return char_tuples_list\n",
    "\n",
    "\n",
    "def indices_dic(char_tuples, words, bigr, trigr):\n",
    "    \"\"\"\n",
    "    This function finds every occurrence of a character name in a list of tokens.\n",
    "    It returns the indices of each occurrence for each character in a numpy array.  \n",
    "    \n",
    "    :param char_tuples: A list of character names as tuples\n",
    "    :param words: A list of single word tokens\n",
    "    :param bigr: A list of bigram tuples \n",
    "    :param trigr: A list of trigram tuples\n",
    "    :return: A dictionary with the characters' names as keys and a array of indices as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {}\n",
    "    for tup in char_tuples:\n",
    "        char_name = \" \".join(tup)\n",
    "        n = len(tup)\n",
    "        if n == 1:\n",
    "            indices = [i for i, x in enumerate(words) if x == tup]\n",
    "        elif n == 2:\n",
    "            indices = [i for i, x in enumerate(bigr) if x == tup]\n",
    "        elif n == 3:\n",
    "            indices = [i for i, x in enumerate(trigr) if x == tup]\n",
    "        dic[char_name] = np.array(indices)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "\n",
    "def links_dic_f(indices_dic, threshold):\n",
    "    \"\"\"\n",
    "    This function creates the dictionary of the links of each character. The function compares \n",
    "    the distance of two indices where character names appear. If this distance is lower than a \n",
    "    threshold it counts as an interaction between the two characters.\n",
    "    \n",
    "    :param indices_dic: The dictionary with the indices for each character \n",
    "    :param threshold: The distance threshold. If the difference of two indices of two characters is \n",
    "                       lower than the threshold, this counts as an interaction\n",
    "    :return: A nested dictionary. Each key is a character name. The values are dictionaries with \n",
    "              keys the characters the initial key character has interacted with, and with values the \n",
    "              number of interactions. \n",
    "    \"\"\"\n",
    "    \n",
    "    link_dic = {}\n",
    "    for first_char, ind_arr1 in indices_dic.items():\n",
    "        dic = {}\n",
    "        for second_char, ind_arr2 in indices_dic.items():\n",
    "            \n",
    "            # Don't count interactions with self\n",
    "            if first_char == second_char:\n",
    "                continue\n",
    "            \n",
    "            matr = np.abs(ind_arr1[np.newaxis].T - ind_arr2) <= threshold\n",
    "            s = np.sum(matr)\n",
    "            \n",
    "            # Only include character pairs with more than 3 interactions\n",
    "            if s > 3:\n",
    "                dic[second_char] = s\n",
    "        link_dic[first_char] = dic\n",
    "    \n",
    "    return link_dic\n",
    "\n",
    "\n",
    "char_tuples = char_tuple_f(chars)\n",
    "ind_dic = indices_dic(char_tuples, tokens, bigrms, trigrms)\n",
    "grand_dic = links_dic_f(ind_dic, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nickname(dic, mainname, nickname):\n",
    "    \"\"\"\n",
    "    This function updates a link dictionary by merging the interaction values of one character \n",
    "    name with another. The second character is considered as a nickname of first, so we wish to \n",
    "    have all character's interactions under one name.\n",
    "    \n",
    "    :param dic: A link dictionary produced by the links_dic_f function\n",
    "    :param mainname: The main name of the character that will remain after the merge\n",
    "    :param nickname: The nickname of the character that will be merge into the main name\n",
    "    :return: A link dictionary like the one produced by links_dic_f but with the nickname \n",
    "              values merged into the main name\n",
    "    \"\"\"\n",
    "    \n",
    "    for key in dic[nickname]:\n",
    "        if key in dic[mainname]:\n",
    "            dic[mainname][key] += dic[nickname][key]\n",
    "        elif key==mainname:\n",
    "            continue\n",
    "        else:\n",
    "            dic[mainname][key] = dic[nickname][key]\n",
    "\n",
    "    for key in dic:\n",
    "        if key==mainname:\n",
    "            dic[key].pop(nickname, None)\n",
    "            continue\n",
    "\n",
    "        if nickname in dic[key]:\n",
    "            if mainname in dic[key]:\n",
    "                dic[key][mainname] += dic[key][nickname]\n",
    "            else:\n",
    "                dic[key][mainname] = dic[key][nickname]\n",
    "\n",
    "        dic[key].pop(nickname, None)\n",
    "\n",
    "    dic.pop(nickname, None)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "\n",
    "def merge_all_nicknames(dic, nickname_list):\n",
    "    \"\"\"\n",
    "    This function updates a link dictionary by merging the interactions of some characters with\n",
    "    other characters. This is done by applying the merge_nickname function for a provided\n",
    "    list of main names and nick names.\n",
    "\n",
    "    :param dic: A link dictionary produced by the links_dic_f function\n",
    "    :param nickname_list: A list of tuples. The first item of the tuple is the main name of\n",
    "                           a character and the second in the nickname\n",
    "    :return: An updated link dictionary with all the nicknames merged into the main names\n",
    "    \"\"\"\n",
    "    \n",
    "    for tup in nickname_list:\n",
    "        (mainname, nickname) = tup\n",
    "        dic = merge_nickname(dic, mainname, nickname)\n",
    "    return dic\n",
    "\n",
    "\n",
    "# Uncomment this part and provide a nickname list if your text contains characters with more than one name\n",
    "\n",
    "#nick_list = []\n",
    "#grand_dic = merge_all_nicknames(grand_dic, nick_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove characters with no interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_link_chars(link_dic, chars_list):\n",
    "    \"\"\"\n",
    "    This function removes all characters with no links from a characters list.\n",
    "    \n",
    "    :param link_dic: A link dictionary produced by the links_dic_f function\n",
    "    :param chars_list: A list of characters\n",
    "    :return: A list of characters. All of the characters in the final list have links with \n",
    "             other characters in the link dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    rem_set = set()\n",
    "    for key in link_dic:\n",
    "        if link_dic[key] == {}:\n",
    "            rem_set.add(key)\n",
    "    \n",
    "    fin_list = [char for char in chars_list if char not in rem_set]\n",
    "    \n",
    "    return fin_list\n",
    "\n",
    "\n",
    "new_chars = remove_zero_link_chars(grand_dic, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the nodes and edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_tuples_f(link_dic):\n",
    "    \"\"\"\n",
    "    It produces the edges of a graph, along with their weight, based on the dictionary\n",
    "    with the interactions of the characters.\n",
    "    \n",
    "    :param link_dic: A link dictionary produced by the links_dic_f function\n",
    "    :return: A list of tuples to be used as the edges of a graph. The first two items are \n",
    "              the nodes of the edge. The third item is the weight of the edge \n",
    "    \"\"\"\n",
    "    \n",
    "    edges_tuples = []\n",
    "    for key in link_dic:\n",
    "        for item, value in link_dic[key].items():\n",
    "            tup = (key.title(), item.title(), value/100)\n",
    "            edges_tuples.append(tup)\n",
    "\n",
    "    return edges_tuples\n",
    "\n",
    "\n",
    "edges_tuples = edge_tuples_f(grand_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_capitals(char_list):\n",
    "    \"\"\"\n",
    "    Capitalizes characters' names.\n",
    "    \n",
    "    :param char_list: A list of characters names\n",
    "    :return: A list of capitalized characters names\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_list = []\n",
    "    for char in char_list:\n",
    "        conv_list.append(char.title())\n",
    "    \n",
    "    return conv_list\n",
    "\n",
    "\n",
    "node_chars = convert_to_capitals(new_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#Add the nodes\n",
    "G.add_nodes_from(node_chars)\n",
    "\n",
    "# Add the edges\n",
    "G.add_weighted_edges_from(edges_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Detect the communities of the graph\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "# Set the community partition as an attribute of the nodes of the graph\n",
    "nx.set_node_attributes(G, partition, 'group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export .gexf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
